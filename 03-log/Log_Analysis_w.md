# Log Analysis (Nginx, History, Squid, Payments)

##  0. Outline

- [1. What is Log Analysis?](#1-what-is-log-analysis)
- [2. Tools for Log Analysis](#2-tools-for-log-analysis)
- [3. Nginx, Squid, and Payment Logs](#3-nginx-squid-and-payment-logs)
- [4. History and Database Analysis](#4-history-and-database-analysis)
- [5. Citations](#5-citations)

## 1. What is Log Analysis?
- Log analysis is the process of examining and interpreting log files generated by various systems, applications, or devices to gain insights, identify patterns, troubleshoot issues, and enhance security.
- Logs are usually stored in plain text format or custom formats and contain information about timestamps, IDs, IP addresses, errors, and other activities that occur within a system.
- Common NCL log files include SSH logs, web server logs, database logs, and application logs.

## 2. Tools for Log Analysis

### `grep`
- Used to search for specific patterns or keywords in log files.
- flags:
  - `-i`: Ignore case sensitivity.
  - `-E`: Use extended regular expressions.
  - `-v`: Invert the match (show lines that do not match).
  - `-o`: Print only the matched parts of a line.
- Example: 
  - `grep -i "error" access.log`: returns lines containing "error" (case-insensitive) in the access.log file.
  - `grep -E "404|500" access.log`: searches for lines containing either "404" or "500" in the access.log file.
  - `grep -oE "http[s]?://[^\s]+" access.log`: extracts URLs from the access.log file. 

### `awk`
- A powerful text processing tool that can be used to manipulate and analyze log files.
- flags:
  - `-F`: Specifies the field separator (e.g., space, comma).
  - `'{print $1, $3}'`: Specifies the fields to print (e.g., first and third fields).
- Example: 
  - `awk '{print $1, $3}' access.log`: prints the first and third fields of each line in the access.log file.
  - `awk '{if ($3 == "error") print $0}' access.log`: prints lines where the third field is "error".
  - `awk -F ',' '"200$" {print $1}' access.log`: prints the first field of lines where the last field ends with "200".
  - `awk -F ',' '$3 > 1000 && $4 == "active" {print $1}' access.log`: prints the first field of lines where the third field is greater than 1000 and the fourth field is "active".
  - `awk '{sum += $3} END {print sum}' access.log`: calculates the sum of the values in the third field of the access.log file.

### `sed`
- A stream editor used for filtering and transforming text in log files.
- flags:
  - `-i`: Edit files in place.
  - `-nr`: Suppresses automatic printing of pattern space and allows the use of extended regular expressions.
  - `s/`: Substitutes a pattern with a replacement string.
  - `/g`: Global replacement (replaces all occurrences in a line).
  - `/d`: Deletes lines matching a pattern.
  - `/p`: Prints lines matching a pattern.
  - `(pattern)`: Captures a group for use in the replacement string.
  - `\1`, `\2`, etc.: References captured groups in the replacement string.
- Example:
  - `sed 's/error/ERROR/g' access.log`: replaces all occurrences of "error" with "ERROR" in the access.log file.
  - `sed -nr 's/.*(http[s]?:\/\/[^\s]+).*/\1/p' access.log`: extracts URLs from the access.log file using a regular expression.
  - `sed -nr 's/.*\[(.*)\].*/\1/p' access.log`: extracts timestamps from the access.log file using a regular expression.

### `cut`
- Used to extract specific fields from log files based on delimiters.
- flags:
  - `-d`: Specifies the delimiter (e.g., space, comma).
  - `-f`: Specifies the field(s) to extract.
- Example: 
  - `cut -d ' ' -f 1,3 access.log`: extracts the first and third fields from the access.log file, using space as the delimiter.

### `head` & `tail`
- Used to display the first or last few lines of a file.
- Default is 10 lines, but can be specified with the `-n` flag.

### `sort`
- Used to sort log entries based on specific fields.
- flags:
  - `-k`: Specifies the field to sort by.
  - `-n`: Sorts numerically.
  - `-r`: Sorts in reverse order.
- Example: 
  - `sort -k 3 access.log`: sorts the access.log file based on the third field.
  - `sort access.log`: sorts the access.log file numerically.

### `uniq`
- Used to filter out duplicate log entries. Data must be sorted before using `uniq` for it to work correctly.
- flags:
  - `-c`: Prefixes lines with the count of occurrences.
  - `-d`: Only prints duplicate lines.
  - `-u`: Only prints unique lines.
- Example: 
  - `uniq access.log`: filters out duplicate lines from the access.log file.
  - `uniq -c access.log`: counts the occurrences of each unique line in the access.log file.

### `wc`
- Used to count the number of lines, words, or characters in log files.
- flags:
  - `-l`: Counts lines.
- Example: 
  - `wc access.log`: counts the number of lines, words, and characters in the access.log file

### Piping and Redirection
- Piping (`|`) allows you to combine multiple commands, passing the output of one command as input to another.
- Redirection (`>`, `>>`) allows you to save the output of a command to a file.
- Example:
  - `grep "error" access.log | wc -l`: counts the number of lines containing "error" in the access.log file.
  - `awk '{print $1}' access.log | sort | uniq > ip_addresses.txt`: extracts the first field (e.g., IP addresses) from the access.log file, sorts them, removes duplicates, and saves the result to ip_addresses.txt.

## 3. Nginx, Squid, and Payment Logs




